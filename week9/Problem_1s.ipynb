{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7ec17618a4178d9dfee8108ec5b1a61",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 9 Problem 1\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says YOUR CODE HERE. Do not write your answer in anywhere else other than where it says YOUR CODE HERE. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select Kernel, and restart the kernel and run all cells (Restart & Run all).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select File → Save and CheckPoint)\n",
    "\n",
    "5. When you are ready to submit your assignment, go to Dashboard → Assignments and click the Submit button. Your work is not submitted until you click Submit.\n",
    "\n",
    "6. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded.\n",
    "\n",
    "7. If your code does not pass the unit tests, it will not pass the autograder.\n",
    "\n",
    "**NOTE:** Validation may take some time. Be patient!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eb3f5b8d6d0fdd23adb60b41ce95102c",
     "grade": false,
     "grade_id": "names",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Author: Apurv Garg\n",
    "### Primary Reviewer: John Nguyen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f85750f7fb93abf0d0482fc9c296d010",
     "grade": false,
     "grade_id": "due_date",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Due Date: 6 PM, March 26, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "edbf616bb1337bc3fd0e42c27ea70d7f",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Display all plots inline\n",
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections as cl\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=2, width=80, compact=True)\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse as sp\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal\n",
    "from pandas.util.testing import assert_frame_equal, assert_index_equal\n",
    "from nose.tools import assert_false, assert_equal, assert_almost_equal, assert_true, assert_in, assert_is_not\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "\n",
    "# We do this to ignore several specific warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set default seaborn plotting style\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d7bcdd973e8ffe77613f5fb74c355d5d",
     "grade": false,
     "grade_id": "data",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Dataset\n",
    "\n",
    "We will analyze the twenty newsgroup data set. We will be analyzing a posting which follows similar structure to an email. We have removed the headers, quotes and footers, i.e. we will just be analyzing the message. Note that we will be performing our analysis on just one message inorder to have computational feasibility.\n",
    "\n",
    "The cell below will create a subdirectory under home called `temp_data`. *If you want to the delete the temp_data directory at any point, run this code in a new cell.*  \n",
    "``` bash\n",
    "! rm -rf /home/data_scientist/temp_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02e364d8bb5b028c91643b1ef29cc01c",
     "grade": false,
     "grade_id": "directory",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "! mkdir ~/temp_data\n",
    "HOME = '/home/data_scientist/temp_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "89bcf3992bba61ef3d4722a5d99c1c00",
     "grade": false,
     "grade_id": "data_load",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "text = fetch_20newsgroups(HOME, remove =('quotes', 'headers', 'footers'))\n",
    "messageID = 11\n",
    "message = text['data'][messageID]\n",
    "target = text['target'][messageID]\n",
    "#print(f'Target Newsgroup: {text[\"target_names\"][target]}')\n",
    "#print(80*'-')\n",
    "#print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf7dd516f2308f0e771da11d674c6fa8",
     "grade": false,
     "grade_id": "problem1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "For this problem, complete the function `string_tokenizer` which will take 3 parameters: `pattern, msg and one_letter`. <br>\n",
    "- In this function, we will explicitly split the text into tokens and then create a `Counter` to accumulate the number of unique occurrences of each token. \n",
    "- Remember to convert the words to lowercase before tokenizing. Since we will just be looking for alphanumeric string, use `re.sub(pattern, ' ', msg)` for removing punctuation tokens. <br>\n",
    "- If parameter `one_letter` is True, then include 1 letter words in the count, else don't include one letter words.\n",
    "\n",
    "**Example:** 5 most common values with `one_letter=False` are: `[('the', 37), ('that', 16), ('of', 14), ('to', 13), ('is', 12)]`<br>\n",
    "5 most common values with `one_letter=True` are: `[('the', 37), ('that', 16), ('of', 14), ('to', 13), ('a', 12)]`\n",
    "\n",
    "**HINT:** If you want, you can create your pattern(regex) removing 1 character words or,<br>\n",
    "You can remove one character words after creating the Counter by taking the values of length > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "578432b95889861ebcc231930faf9f6f",
     "grade": false,
     "grade_id": "problem1_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def string_tokenizer(pattern, msg, one_letter):\n",
    "    '''           \n",
    "    Parameters\n",
    "    ----------\n",
    "    pattern : Regular expression searching for punctuations\n",
    "    msg : the message which is to be tokenized\n",
    "    one_letter : A boolean value where True implies that you have to include 1-letter words \n",
    "                 and a False value implies that you have to remove the 1-letter words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A Counter object wc    \n",
    "    '''    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # TOkenize token counts\n",
    "    words = re.sub(pattern, ' ', msg.lower()).split()\n",
    "    \n",
    "    # Accumulate token counts\n",
    "    if one_letter:\n",
    "        wc = cl.Counter(words)\n",
    "    else:\n",
    "        lis = []\n",
    "        for word in words:\n",
    "            if len(word) > 1:\n",
    "                lis.append(word)\n",
    "        wc = cl.Counter(lis)\n",
    "        \n",
    "    return wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8cdc7a00004b741e629807f4933a542a",
     "grade": true,
     "grade_id": "problem1_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[^\\w\\s]')\n",
    "wc1 = string_tokenizer(pattern, message, False)\n",
    "wc2 = string_tokenizer(pattern, message, True)\n",
    "assert_equal(isinstance(wc1, cl.Counter), True)\n",
    "assert_equal(isinstance(wc2, cl.Counter), True)\n",
    "assert_equal(len(wc1), 219)\n",
    "assert_equal(len(wc2), 224)\n",
    "assert_equal(wc1.most_common()[0], ('the', 37))\n",
    "assert_equal(wc2.most_common()[5], ('is', 12))\n",
    "assert_equal(wc1.most_common()[4], ('is', 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ce9bd5bf80892905dd16136c6555fcd0",
     "grade": false,
     "grade_id": "cell-378fdac3ae99477c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term        : Frequency\n",
      "-------------------------\n",
      "the         : 0.081\n",
      "that        : 0.035\n",
      "of          : 0.031\n",
      "to          : 0.029\n",
      "is          : 0.026\n",
      "and         : 0.022\n",
      "in          : 0.022\n",
      "this        : 0.020\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Term':12s}: {'Frequency'}\")\n",
    "print(25*'-')\n",
    "\n",
    "# Compute term counts\n",
    "t_wc1 = sum(wc1.values())\n",
    "\n",
    "# Display counts and frequencies\n",
    "for wt in wc1.most_common(8):\n",
    "    print(f'{wt[0]:12s}: {wt[1]/t_wc1:4.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "857429c70c8ecccd47144dd922fed526",
     "grade": false,
     "grade_id": "problem2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "For this problem, complete the function `vectorize`,which will take 3 parameters: `rm_stop, data and message`. <br>\n",
    "-  Inside the function, create a CountVectorizer object with hyper-parameters: `stop_words = 'english', analyzer='word', lowercase=True` if the condition rm_stop is True and if the condition rm_stop is False, create an object with hyper-parameters: `analyzer='word', lowercase=True`. <br>\n",
    "-  Fit the CountVectorizer created on the data and transform the message to a Document Term Matrix(dtm). <br>\n",
    "-  Find non-zero elements from Document Term Matrix and create a list containing a tuple of Document-Term Matrix[i, j] and Count.<br>\n",
    "-  Finally, find non-zero elements and return a sorted list of **10** elements(tuples) based on word counts(maximum comes 1st).<br>\n",
    "-  Finally return the CountVectorizer object and the list with sorted Document-Term Matrix[i, j] and Count.\n",
    "\n",
    "**Example:** Your sample list should look like :<br>\n",
    "[(0, 88532, 37),(0, 88519, 16),(0, 67670, 14),(0, 89360, 13),(0, 51136, 12),\n",
    "(0, 18521, 10),(0, 49447, 10),(0, 60078, 9),(0, 88767, 9),(0, 69918, 8)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cc8b95e5d4acc11af5d85394549bc0f0",
     "grade": false,
     "grade_id": "problem2_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def vectorize(rm_stop, data, message):\n",
    "    '''           \n",
    "    Name your CountVectorizer as cv and sorted list of tuples as srt_dtm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rm_stop : A boolean value which if True, remove stop words\n",
    "    data : whole data set to build the vocabulary\n",
    "    msg : the message which is to be vectorized\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of 2 containing the CountVectorizer object and the list with sorted Document-Term Matrix[i, j] and Count.\n",
    "    '''    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # Create a CountVectorizer object\n",
    "    if rm_stop:\n",
    "        cv = CountVectorizer(stop_words = 'english', analyzer='word', lowercase=True)\n",
    "    else:\n",
    "        cv = CountVectorizer(analyzer='word', lowercase=True)\n",
    "    \n",
    "    # Fit the cv\n",
    "    cv.fit(data)\n",
    "    \n",
    "    # Create an iteratable to apply transform\n",
    "    msg=[]\n",
    "    msg.append(message)\n",
    "    \n",
    "    # Transform the data to dtm\n",
    "    dtm = cv.transform(msg)\n",
    "    \n",
    "    # Find non-zero elements\n",
    "    i, j, c = sp.find(dtm)\n",
    "    dtm_list = list(zip(i, j, c))\n",
    "    \n",
    "    # Sort dtm list\n",
    "    srt_dtm = sorted(dtm_list,key=itemgetter(2), reverse=True)[:10]\n",
    "\n",
    "    return cv,srt_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4cea94281277e6b005bc83911898b636",
     "grade": true,
     "grade_id": "problem2_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "cv1,srt_dtm1 = vectorize(rm_stop = False, data=text['data'], message=message)\n",
    "cv2,srt_dtm2 = vectorize(rm_stop = True, data=text['data'], message=message)\n",
    "assert_equal(isinstance(cv1,CountVectorizer), True)\n",
    "assert_equal(isinstance(cv2,CountVectorizer), True)\n",
    "assert_equal(srt_dtm1[0], (0, 88532, 37))\n",
    "assert_equal(srt_dtm1[1], (0, 88519, 16))\n",
    "assert_equal(srt_dtm2[0], (0, 69723, 8))\n",
    "assert_equal(srt_dtm2[1], (0, 26952, 6))\n",
    "max_key1 = max(srt_dtm1, key=itemgetter(2))[1]\n",
    "assert_equal(max_key1,88532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "200502ad8e55e6ccf7173fccfdb2e857",
     "grade": false,
     "grade_id": "cell-65024efa0ab37825",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def cnt(top_display, cv , srt_dtm):\n",
    "\n",
    "    terms = cv.vocabulary_\n",
    "    # Sort our document term list, and unzip\n",
    "    i, j, c = zip(*srt_dtm)\n",
    "    # Grab out the keys and values for top terms\n",
    "    x_keys = [(k, v) for k, v in terms.items() \n",
    "              if terms[k] in j[:top_display]]\n",
    "    x_keys.sort(key=itemgetter(1), reverse=True)\n",
    "    # Grab the data, including counts from DTM list\n",
    "    x_counts = srt_dtm[:top_display]\n",
    "    x_counts.sort(key=itemgetter(1), reverse=True)\n",
    "    # Now we merge the two lists so we can sort to display terms in order\n",
    "    x_merged = []\n",
    "    for idx in range(len(x_keys)):\n",
    "        x_merged.append((x_keys[idx][0], \n",
    "                         x_keys[idx][1], \n",
    "                         x_counts[idx][2]))\n",
    "    x_merged.sort(key=itemgetter(2), reverse=True)\n",
    "    print('Count: Term in Vocabulary')\n",
    "    print(40*'-')\n",
    "    for x in x_merged:\n",
    "        print(f'{x[2]:5d}: vocabulary[{x[1]}] = {x[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d0b5a322bb89629c8a3c450600df995",
     "grade": false,
     "grade_id": "cell-22dac0830e44c3d7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: Term in Vocabulary\n",
      "----------------------------------------\n",
      "   37: vocabulary[88532] = the\n",
      "   16: vocabulary[88519] = that\n",
      "   14: vocabulary[67670] = of\n",
      "   13: vocabulary[89360] = to\n",
      "   12: vocabulary[51136] = is\n",
      "   10: vocabulary[18521] = and\n",
      "--------------------------------------------------------------------------------\n",
      "Count: Term in Vocabulary\n",
      "----------------------------------------\n",
      "    8: vocabulary[69723] = parent\n",
      "    6: vocabulary[26952] = child\n",
      "    5: vocabulary[62940] = moral\n",
      "    4: vocabulary[86584] = swear\n",
      "    3: vocabulary[28151] = code\n",
      "    3: vocabulary[16289] = absolute\n"
     ]
    }
   ],
   "source": [
    "cnt(6, cv1, srt_dtm1)\n",
    "print(80*'-')\n",
    "cnt(6, cv2, srt_dtm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "10d2b3bb2d3a0de7795d7447df55ae4b",
     "grade": false,
     "grade_id": "problem3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 3\n",
    "\n",
    "For this problem, complete the function `tokenize_nltk` which will take `pattern` and `msg` as parameters and return lexical diversity, unique tokens, maximum occuring token and 5 _hapaxes_ in the corpus. <br>Use NLTK library to tokenize the message(passed through msg parameter). Also, remember to convert the words to lowercase before tokenizing.\n",
    "Since we will just be looking for alphanumeric string, use `re.sub(pattern, ' ', msg)` for removing punctuation tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "aec272ce2de291e6da23568893ff2e7c",
     "grade": false,
     "grade_id": "problem3_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_nltk(pattern, msg):\n",
    "    '''           \n",
    "    Parameters\n",
    "    ----------\n",
    "    pattern : Regular expression searching for punctuations\n",
    "    msg : the message which is to be tokenized\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of 4 containing the lexical diversity value, number of unique tokens, maximum occuring token,\n",
    "    and a list of 5 containing hapaxes.\n",
    "    '''    \n",
    "    # YOUR CODE HERE\n",
    "    # Tokenize a text document\n",
    "    words = [word.lower() for word in nltk.word_tokenize(re.sub(pattern, ' ', msg))]\n",
    "    \n",
    "    # Count token ovvurances\n",
    "    counts = nltk.FreqDist(words)\n",
    "    \n",
    "    # Compute lexical diversity\n",
    "    num_words = len(words)\n",
    "    num_tokens = len(counts)\n",
    "    lexdiv = num_words / num_tokens\n",
    "    \n",
    "    # Compute number of unique tokens\n",
    "    unique_tk = counts.B()\n",
    "    \n",
    "    # Compute maximum occuring token\n",
    "    max_tk = counts.max()\n",
    "    \n",
    "    # 5 containing hapaxes\n",
    "    haps = counts.hapaxes()[:5]\n",
    "    \n",
    "    return lexdiv, unique_tk, max_tk, haps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "44dec6a995597e5e2beb1b8443584ccf",
     "grade": true,
     "grade_id": "problem3_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "pattern1 = re.compile(r'[^\\w\\s]')\n",
    "div, bins, max_val, hap = tokenize_nltk(pattern1, message)\n",
    "assert_almost_equal(div, 2.13392, 3)\n",
    "assert_equal(bins, 224)\n",
    "assert_equal(max_val, 'the')\n",
    "assert_equal(isinstance(hap, list), True)\n",
    "assert_equal(len(hap), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7afacee59e0c03036196a2c01889457d",
     "grade": false,
     "grade_id": "hap_print",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 hapaxes in corpus are: ['yep', 'pretty', 'much', 'jewish', 'thinking']\n"
     ]
    }
   ],
   "source": [
    "print('5 hapaxes in corpus are:',hap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
