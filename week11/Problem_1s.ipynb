{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8bf137dd9641960b44458688b62ba19a",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 11 Problem 1\n",
    "\n",
    "A few things you should keep in mind when working on assignments:\n",
    "\n",
    "1. Make sure you fill in any place that says YOUR CODE HERE. Do not write your answer in anywhere else other than where it says YOUR CODE HERE. Anything you write anywhere else will be removed or overwritten by the autograder.\n",
    "\n",
    "2. Before you submit your assignment, make sure everything runs as expected. Go to menubar, select Kernel, and restart the kernel and run all cells (Restart & Run all).\n",
    "\n",
    "3. Do not change the title (i.e. file name) of this notebook.\n",
    "\n",
    "4. Make sure that you save your work (in the menubar, select File → Save and CheckPoint)\n",
    "\n",
    "5. When you are ready to submit your assignment, go to Dashboard → Assignments and click the Submit button. Your work is not submitted until you click Submit.\n",
    "\n",
    "6. You are allowed to submit an assignment multiple times, but only the most recent submission will be graded.\n",
    "\n",
    "7. If your code does not pass the unit tests, it will not pass the autograder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "814ff1da7485a13bc060abd7e3cca6b1",
     "grade": false,
     "grade_id": "names",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Author: Apurv Garg\n",
    "### Primary Reviewer: Radhir Kothuri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a2ee047e07c99564f41c915d9dbca721",
     "grade": false,
     "grade_id": "due_date",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Due Date: 6 PM, April 09, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9d85092f656e4ff040ad284a5556513b",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Import email library, policy controls how to process the data\n",
    "import email as em\n",
    "from email import policy\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal\n",
    "from pandas.util.testing import assert_frame_equal, assert_index_equal\n",
    "from nose.tools import assert_false, assert_equal, assert_almost_equal, assert_true, assert_in, assert_is_not\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2db2b37207c17b1fbb905ceac6282c21",
     "grade": false,
     "grade_id": "dataset1_header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Datset(Problem1)\n",
    "\n",
    "For this problem, we will be reading an email from the directory and will find out the main attributes of the email message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0e3777824b9af4c65f4bfffd829bc95",
     "grade": false,
     "grade_id": "dataset1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "with open('/home/data_scientist/data/misc/INFO490.eml') as fin:\n",
    "    msg = em.message_from_file(fin, policy=policy.default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0523feb57f79cbc6ae2b6fdb3d669513",
     "grade": false,
     "grade_id": "problem1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "For this problem, complete the function `vals` which will take `message` as parameter and return the receiver's information(whom the message was sent to), email subject, email date, email content type, email content language, html payload, plain text payload content type and html payload content type in that order. <br>\n",
    "For last 3 return values, check if the email has multiple parts, after which extract different parts (or payloads) from the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "9d858d3a0a2c539fa4783452cac31e1b",
     "grade": false,
     "grade_id": "problem1_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def vals(message):\n",
    "    '''           \n",
    "    Parameters\n",
    "    ----------\n",
    "    message : email message\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of 8 containing receiver's information(whom the message was sent to), \n",
    "    email subject, \n",
    "    email date, \n",
    "    email content type, \n",
    "    email content language, \n",
    "    html payload, \n",
    "    plain text payload content type, \n",
    "    html payload content type\n",
    "    \n",
    "    HINT\n",
    "    ----\n",
    "    'Content-Language' gives the language of the email content\n",
    "    To find different payload content type, explore different get methods\n",
    "    '''    \n",
    "    # YOUR CODE HERE\n",
    "    to = msg['to']\n",
    "    sub = msg['subject']\n",
    "    date = msg['date']\n",
    "    con_tp = msg['content-type']\n",
    "    con_lang = msg['content-language']\n",
    "    \n",
    "    if msg.is_multipart:\n",
    "#         data = msg.get_payload(0).get_content()\n",
    "        data = msg.get_payload(0)\n",
    "        html = msg.get_payload(1)\n",
    "        data_tp = data.get_content_type()\n",
    "        html_pl = html.get_content_type()\n",
    "        \n",
    "    return(to, sub, date, con_tp, con_lang, html, data_tp, html_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2bcfe2b57d21d0a25605c8f882b07e5b",
     "grade": true,
     "grade_id": "problem1_test",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "to, subject, date, content_type, language, html, data_content, html_content = vals(msg)\n",
    "assert_equal(to, '\"Garg, Apurv\" <apurvg2@illinois.edu>')\n",
    "assert_equal(subject, 'INFO490')\n",
    "assert_equal(date, 'Sat, 24 Mar 2018 23:02:35 -0500')\n",
    "assert_equal(subject, 'INFO490')\n",
    "assert_equal(content_type, 'multipart/alternative; boundary=\"B_3604777369_1001855260\"')\n",
    "assert_equal(language, 'en-US')\n",
    "assert_equal(data_content, 'text/plain')\n",
    "assert_equal(html_content, 'text/html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb024e6869ea1fc530e19deefa63917c",
     "grade": false,
     "grade_id": "cell-45075994cde7bac1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html xmlns:o=\"urn:schemas-microsoft-com:office:office\" xmlns:w=\"urn:schemas-microsoft-com:office:word\" xmlns:m=\"http://schemas.microsoft.com/office/2004/12/omml\" xmlns=\"http://www.w3.org/TR/REC-html40\">\n",
       "<head>\n",
       "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n",
       "<meta name=\"Title\" content=\"\">\n",
       "<meta name=\"Keywords\" content=\"\">\n",
       "<meta name=\"Generator\" content=\"Microsoft Word 15 (filtered medium)\">\n",
       "<style><!--\n",
       "/* Font Definitions */\n",
       "@font-face\n",
       "\t{font-family:\"Cambria Math\";\n",
       "\tpanose-1:2 4 5 3 5 4 6 3 2 4;}\n",
       "@font-face\n",
       "\t{font-family:Calibri;\n",
       "\tpanose-1:2 15 5 2 2 2 4 3 2 4;}\n",
       "/* Style Definitions */\n",
       "p.MsoNormal, li.MsoNormal, div.MsoNormal\n",
       "\t{margin:0in;\n",
       "\tmargin-bottom:.0001pt;\n",
       "\tfont-size:12.0pt;\n",
       "\tfont-family:\"Calibri\",sans-serif;}\n",
       "a:link, span.MsoHyperlink\n",
       "\t{mso-style-priority:99;\n",
       "\tcolor:#0563C1;\n",
       "\ttext-decoration:underline;}\n",
       "a:visited, span.MsoHyperlinkFollowed\n",
       "\t{mso-style-priority:99;\n",
       "\tcolor:#954F72;\n",
       "\ttext-decoration:underline;}\n",
       "span.EmailStyle17\n",
       "\t{mso-style-type:personal-compose;\n",
       "\tfont-family:\"Calibri\",sans-serif;\n",
       "\tcolor:windowtext;}\n",
       "span.msoIns\n",
       "\t{mso-style-type:export-only;\n",
       "\tmso-style-name:\"\";\n",
       "\ttext-decoration:underline;\n",
       "\tcolor:teal;}\n",
       ".MsoChpDefault\n",
       "\t{mso-style-type:export-only;\n",
       "\tfont-family:\"Calibri\",sans-serif;}\n",
       "@page WordSection1\n",
       "\t{size:8.5in 11.0in;\n",
       "\tmargin:1.0in 1.0in 1.0in 1.0in;}\n",
       "div.WordSection1\n",
       "\t{page:WordSection1;}\n",
       "--></style>\n",
       "</head>\n",
       "<body bgcolor=\"white\" lang=\"EN-US\" link=\"#0563C1\" vlink=\"#954F72\">\n",
       "<div class=\"WordSection1\">\n",
       "<p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">Good evening, Statisticians!<o:p></o:p></span></p>\n",
       "<p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">&nbsp;<o:p></o:p></span></p>\n",
       "<p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">The&nbsp;<i>p</i>-value is used in the context of&nbsp;null hypothesis&nbsp;testing in order to quantify the idea of&nbsp;statistical significance&nbsp;of evidence. In essence, a claim is assumed valid if its counter-claim is improbable.<o:p></o:p></span></p>\n",
       "<p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">As such, the only hypothesis that needs to be specified in this test and which embodies the counter-claim is referred to as the&nbsp;null hypothesis&nbsp;(that is, the hypothesis to be nullified). A result is said to\n",
       " be statistically significant if it allows us to reject the null hypothesis. That is, as per the reduction and absurd reasoning, the statistically significant result should be highly improbable if the null hypothesis is assumed to be true. The rejection of\n",
       " the null hypothesis implies that the correct hypothesis lies in the logical complement of the null hypothesis. However, unless there is a single alternative to the null hypothesis, the rejection of null hypothesis does not tell us which of the alternatives\n",
       " might be the correct one.<o:p></o:p></span></p>\n",
       "<p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">&nbsp;<o:p></o:p></span></p>\n",
       "<p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">Regards,<o:p></o:p></span></p>\n",
       "<p class=\"MsoNormal\"><span style=\"font-size:11.0pt\">XXXYYYZZZ<o:p></o:p></span></p>\n",
       "<p class=\"MsoNormal\"><span style=\"font-size:11.0pt\"><o:p>&nbsp;</o:p></span></p>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(html.get_content())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f60bcaf32ad46fa88be5ea11210ab415",
     "grade": false,
     "grade_id": "dataset2_head",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Dataset(Problem 2 and 3)\n",
    "\n",
    "For the next 2 problems, we will be classifying the mails into spam and ham. Though Naive Bayes' is the most commonly used technique for the spam-ham classification, we will be trying out RandomForest and SGDClassifier in the next 2 problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f4c163c273d1a1ffcda010bcc27c8bf",
     "grade": false,
     "grade_id": "dataset2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# First we find our HOME directory\n",
    "home_dir = !echo $HOME\n",
    "\n",
    "# Define data directory\n",
    "home = home_dir[0] +'/data/'\n",
    "mypath = home + 'email/'\n",
    "\n",
    "ham = []\n",
    "spam = []\n",
    "\n",
    "# Max number of files to read\n",
    "max_files = 500\n",
    "\n",
    "# Read in good (ham) emails\n",
    "for root, dirs, files in os.walk(os.path.join(mypath, 'ham')):\n",
    "    for count, file in enumerate(files):\n",
    "    \n",
    "        # To control memory usage, we limit the number of files\n",
    "        if count >= max_files:\n",
    "            break\n",
    "            \n",
    "        with open(os.path.join(root, file), encoding='ISO-8859-1') as fin:\n",
    "            msg = em.message_from_file(fin, policy=policy.default)\n",
    "            for part in msg.walk():\n",
    "                if part.get_content_type() == 'text/plain':\n",
    "                    data = part.get_payload(None, decode=True)\n",
    "\n",
    "            ham.append(data.decode(encoding='ISO-8859-1'))\n",
    "\n",
    "# Read in bad (spam) emails\n",
    "for root, dirs, files in os.walk(os.path.join(mypath, 'spam')):\n",
    "    for count, file in enumerate(files):\n",
    "        \n",
    "        # To control memory usage, we limit the number of files\n",
    "        if count >= max_files:\n",
    "            break\n",
    "           \n",
    "        with open(os.path.join(root, file), encoding='ISO-8859-1') as fin:\n",
    "            msg = em.message_from_file(fin, policy=policy.default)\n",
    "            for part in msg.walk():\n",
    "                if part.get_content_type() == 'text/plain':\n",
    "                    data = part.get_payload(None, decode=True)\n",
    "\n",
    "            spam.append(data.decode(encoding='ISO-8859-1'))\n",
    "\n",
    "# For text analysis, we need NumPy arrays.\n",
    "# Convert the text lists to NumPy arrays\n",
    "\n",
    "pos_emails = np.array(ham)\n",
    "neg_emails = np.array(spam) \n",
    "\n",
    "# Create label arrays\n",
    "pos_labels = np.ones(pos_emails.shape[0])\n",
    "neg_labels = np.zeros(neg_emails.shape[0])\n",
    "# We split positive/negative emails into two groups test/train each. \n",
    "# This value must be less than max_files.\n",
    "split_value = 100\n",
    "\n",
    "# We combine neg and positive into four arrays.\n",
    "x_train = np.concatenate((pos_emails[:split_value], \n",
    "                          neg_emails[:split_value]), axis = 0)\n",
    "\n",
    "x_test = np.concatenate((pos_emails[split_value:],\n",
    "                         neg_emails[split_value:]), axis = 0)\n",
    "\n",
    "y_train = np.concatenate((pos_labels[:split_value], \n",
    "                          neg_labels[:split_value]), axis = 0)\n",
    "\n",
    "y_test = np.concatenate((pos_labels[split_value:],\n",
    "                         neg_labels[split_value:]), axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8a8ef3c5224d805790ecfb2d938de71c",
     "grade": false,
     "grade_id": "problem2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "For this problem, complete the function `rf_class` which will take `x_train`, `y_train` and `x_test` as parameters and return the pipeline object(with a TfidfVectorizer and RandomForestClassifier) and predicted value for test set.\n",
    "\n",
    "- The parameters to be used for the TfidfVectorizer are `stop_words`='english', `ngram_range`=(2,2), `lowercase`=True and `min_df`=2. Other parameters should be kept as default. \n",
    "- Create a RandomForestClassifier with parameters: `min_samples_split`=5 and `random_state`=40. Other parameters are left to be default. <br>\n",
    "- Fit estimator to training data after vectorizing and predict for the test dataset using this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e7d1ebf3245bb7cf3a0c23cca7ee5930",
     "grade": false,
     "grade_id": "problem2_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def rf_class(x_train, y_train, x_test):\n",
    "    '''           \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train : training set features\n",
    "    y_train : training set labels\n",
    "    x_test : test set features\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of 2 containing the pipeline object(with a TfidfVectorizer and RandomForestClassifier), \n",
    "    predicted value for test set \n",
    "    ''' \n",
    "    # YOUR CODE HERE\n",
    "    steps = [('TfidfVectorizer',TfidfVectorizer(stop_words='english', ngram_range=(2,2), min_df=2)),\\\n",
    "             ('RandomForestClassifier', RandomForestClassifier(min_samples_split=5, random_state=40))]\n",
    "             \n",
    "    pipeline = Pipeline(steps)\n",
    "             \n",
    "    pipeline.fit(x_train, y_train)\n",
    "    pred = pipeline.predict(x_test)         \n",
    "             \n",
    "    return pipeline, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1fd6f1e05dc4d3a6d5a263d65e9e148a",
     "grade": true,
     "grade_id": "problem2_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "pipeline1, y_pred1 = rf_class(x_train, y_train, x_test)\n",
    "val1, val2 = pipeline1.named_steps.items()\n",
    "tf = val1[0]\n",
    "rf = val2[0]\n",
    "assert_equal(isinstance(pipeline1, Pipeline), True)\n",
    "assert_equal(isinstance(pipeline1.named_steps[rf], RandomForestClassifier), True)\n",
    "assert_equal(isinstance(pipeline1.named_steps[tf], TfidfVectorizer), True)\n",
    "assert_equal(pipeline1.named_steps[tf].get_params()['stop_words'], 'english')\n",
    "assert_equal(pipeline1.named_steps[tf].get_params()['ngram_range'], (2,2))\n",
    "assert_equal(pipeline1.named_steps[tf].get_params()['lowercase'], True)\n",
    "assert_equal(pipeline1.named_steps[tf].get_params()['min_df'], 2)\n",
    "assert_equal(pipeline1.named_steps[rf].get_params()['min_samples_split'], 5)\n",
    "assert_almost_equal(100.0 * metrics.accuracy_score(y_test, y_pred1), 97.625, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "856043f1a26e70bc2fbea3b6af013d2c",
     "grade": false,
     "grade_id": "class2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Ham       0.96      0.99      0.98       400\n",
      "       Spam       0.99      0.96      0.98       400\n",
      "\n",
      "avg / total       0.98      0.98      0.98       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred1, \n",
    "                                    target_names = ['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "76ebb00a8b4f74afa8d14d4a5298a124",
     "grade": false,
     "grade_id": "problem3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Problem 3\n",
    "\n",
    "For this problem, complete the function `sgd_class` which will take `x_train`, `y_train` and `x_test` as parameters and return the pipeline object(with a CountVectorizer and SGDClassifier) and predicted value for test set.\n",
    "\n",
    "- The parameters to be used for the CountVectorizer are `stop_words`='english', `ngram_range`=(1,3) and `lowercase`=True. Other parameters should be kept as default. \n",
    "- Create a SGDClassifier with parameters: `penalty`='l1' and `random_state`=40. Other parameters are left to be default. <br>\n",
    "- Fit estimator to training data after vectorizing and predict for the test dataset using this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "59bf7501fa3ad9bb286b79ccf9334e9b",
     "grade": false,
     "grade_id": "problem3_answer",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sgd_class(x_train, y_train, x_test):\n",
    "    '''           \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train : training set features\n",
    "    y_train : training set labels\n",
    "    x_test : test set features\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of 2 containing the pipeline object(with a CountVectorizer and SGDClassifier), \n",
    "    predicted value for test set \n",
    "    '''    \n",
    "    # YOUR CODE HERE\n",
    "    pipeline = Pipeline([('CountVectorizer',CountVectorizer(stop_words='english',\\\n",
    "                        ngram_range=(1,3), lowercase=True)), ('SGDClassifier', \\\n",
    "                        SGDClassifier(penalty='l1', random_state=40))])\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    \n",
    "    pred = pipeline.predict(x_test)\n",
    "    \n",
    "    return pipeline, pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "77e2cbe9c81b4b12bc840ffab319e4c3",
     "grade": true,
     "grade_id": "problem3_tests",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "pipeline2, y_pred2 = sgd_class(x_train, y_train, x_test)\n",
    "val1, val2 = pipeline2.named_steps.items()\n",
    "cv = val1[0]\n",
    "sgd = val2[0]\n",
    "assert_equal(isinstance(pipeline2, Pipeline), True)\n",
    "assert_equal(isinstance(pipeline2.named_steps[cv], CountVectorizer), True)\n",
    "assert_equal(isinstance(pipeline2.named_steps[sgd], SGDClassifier), True)\n",
    "assert_equal(pipeline2.named_steps[cv].get_params()['stop_words'], 'english')\n",
    "assert_equal(pipeline2.named_steps[cv].get_params()['ngram_range'], (1,3))\n",
    "assert_equal(pipeline2.named_steps[cv].get_params()['lowercase'], True)\n",
    "assert_equal(pipeline2.named_steps[sgd].get_params()['penalty'], 'l1')\n",
    "assert_almost_equal(100.0 * metrics.accuracy_score(y_test, y_pred2), 91.000000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "da4f0382ad482c58bebceb718337c7e7",
     "grade": false,
     "grade_id": "class3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        Ham       0.99      0.83      0.90       400\n",
      "       Spam       0.85      0.99      0.92       400\n",
      "\n",
      "avg / total       0.92      0.91      0.91       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred2, \n",
    "                                    target_names = ['Ham', 'Spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
